<section xml:id="sec-Determinants" xmlns:xi="http://www.w3.org/2001/XInclude">

	<title>
		Determinants
	</title>
	<introduction>
		<p> In this section we introduce the determinant of a matrix and describe some of its properties.  The determinant is defined through a recursive process, and as a result many of the theorems in this section involve proofs by mathematical induction.  Since that technique is beyond the scope of this course we will omit most of the proofs in this section. </p>
	</introduction>
	<subsection>
		<title> Cofactor expansion </title>
		<p> The definition we are about to give will seem quite strange and unmotivated.  You might want to look ahead to <xref ref="thm-fundamental-determinant" /> and <xref ref="subsec-geometric-determinant" /> to see why determinants are important before you work through the details of the definition. </p>
		<definition xml:id="def-2x2-determinant">
			<p> If <m>A = \begin{bmatrix}a\end{bmatrix}</m> is a <m>1 \times 1</m> matrix then <m>\det(A) = a</m>.</p>
			<p> If <m>A = \begin{bmatrix}a \amp b \\ c \amp d\end{bmatrix}</m> is a <m>2 \times 2</m> matrix then <m>\det(A) = ad-bc</m>.</p>
		</definition>
		<p> Notice that in both cases of the definition <m>\det(A)</m> is a <em>number</em>.  There are formulas for determinants of larger matrices, but they are quite unpleasant, so instead we will describe a method for the computation of the determinant.</p>
		<definition>
			<p> Let <m>A</m> be an <m>n \times n</m> matrix, and suppose that <m>1 \leq i \leq n</m> and <m>1 \leq j \leq n</m>.  The <em><m>(i,j)</m>-minor</em> of <m>A</m> is the <m>(n-1) \times (n-1)</m> matrix <m>A_{i,j}</m> defined by deleting row <m>i</m> and column <m>j</m> of <m>A</m>.</p>
		</definition>
		<example>
			<p> If <m>A = \begin{bmatrix}3 \amp 6 \amp -2 \\ 2 \amp -3 \amp 0 \\ 1 \amp 2 \amp 3\end{bmatrix}</m> then <m>A_{1,2} = \begin{bmatrix}2 \amp 0 \\ 1 \amp 3\end{bmatrix}</m>.</p>
		</example>
		<definition>
			<p>If <m>A = \begin{bmatrix}a_{1,1} \amp a_{1,2} \amp a_{1,3} \amp \cdots \amp a_{1,n} \\ a_{2,1} \amp a_{2,2} \amp a_{2,3} \amp \cdots \amp a_{2,n} \\ a_{3,1} \amp a_{3,2} \amp a_{3,3} \amp \cdots \amp a_{3,n} \\ \vdots \amp \vdots \amp \vdots \amp \ddots \amp \vdots \\ a_{n,1} \amp a_{n,2} \amp a_{n,3} \amp \cdots \amp a_{n,n}\end{bmatrix}</m> is an <m>n \times n</m> matrix then <me>\det(A) = a_{1,1}\det(A_{1,1}) - a_{1,2}\det(A_{1,2}) + a_{1,3}\det(A_{1,3}) - \cdots \pm a_{1,n}\det(A_{1,n})</me>.</p>
			<p> This method of computing <m>\det(A)</m> is referred to as <em>cofactor expansion</em>.</p>
		</definition>

		<note><p>Some authors use the symbol <m>\abs{A}</m> to refer to the determinant of <m>A</m>.  Since we already have plenty of things denoted by vertical bars, we will always use the notation <m>\det(A)</m>.</p></note>
		<example>
			<p> Let <m>A = \begin{bmatrix}3 \amp 6 \amp -2 \\ 2 \amp -3 \amp 0 \\ 1 \amp 2 \amp 3\end{bmatrix}</m>.  Then:</p>
			<md>
				<mrow>\det(A) \amp = 3\det(A_{1,1}) - 6\det(A_{1,2}) + (-2)\det(A_{1,3}) </mrow>
				<mrow>\amp = 3\det\left(\begin{bmatrix}-3 \amp 0 \\ 2 \amp 3\end{bmatrix}\right) - 6\det\left(\begin{bmatrix}2 \amp 0 \\ 1 \amp 3\end{bmatrix}\right) + (-2)\det\left(\begin{bmatrix}2 \amp -3 \\ 1 \amp 2\end{bmatrix}\right)</mrow>
				<mrow>\amp = 3((-3)(3)-(2)(0)) -6((2)(3) - (1)(0)) + (-2)((2)(2) - (1)(-3))</mrow>
				<mrow>\amp = -77</mrow>
			</md>
		</example>

		<p> As you can see, computing the determinant of a <m>3 \times 3</m> matrix involves computing <m>3</m> determinants of <m>2 \times 2</m> matrices.  Similarly, computing the determinant of a <m>4 \times 4</m> matrix involves computing <m>4</m> determinants of <m>3 \times 3</m> matrices, for a total of <m>4\cdot 3 = 12</m> computations of <m>2 \times 2</m> determinants.  For a <m>5 \times 5</m> matrix we need to compute <m>5</m> determinants of <m>4 \times 4</m> matrices, for a total of <m>60</m> determinants of <m>2 \times 2</m> matrices.  This very quickly gets out of hand!</p>
		<p> If we are lucky and <m>a_{1,j} = 0</m> for some <m>j</m> then we can skip computing <m>\det(A_{1,j})</m>, because in the cofactor expansion the term <m>a_{1,j}\det(A_{1,j})</m> will be <m>0</m>.  Fortunately, it is possible to do cofactor expansion along <em>any</em> row, or indeed any column, of <m>A</m>, so we can "target" rows and columns that have many <m>0</m> entries.</p>
		<fact xml:id="fact-determinant-signs"><p>
		The only difference in using a row or column other than row <m>1</m> is that the sign pattern in the cofactor expansion may change.  Specifically, if we do cofactor expansion along an <em>odd-numbered row or column</em> then the sign of the first term of the expansion will be positive (and alternating signs thereafter), while if we do cofactor expansion along an <em>even-numbered row or column</em> then the sign of the first term of the expansion will be negative (and again, alternating signs thereafter). 
		</p></fact>

		<example xml:id="ex-determinant-signs">
			<p> Let <m>A = \begin{bmatrix}1 \amp 2 \amp 3 \amp 4 \\ 1 \amp 0 \amp 2 \amp 1 \\ 5 \amp 0 \amp 4 \amp 4 \\ -2 \amp 1 \amp 3 \amp 7\end{bmatrix}</m>.  If we do cofactor expansion along the first row, we find:</p>
			<me> \det(A) = 1\det\begin{bmatrix}0 \amp 2 \amp 1 \\ 0 \amp 4 \amp 0 \\ 1 \amp 3 \amp 7\end{bmatrix} -2\det\begin{bmatrix}1 \amp 2 \amp 1 \\ 5 \amp 4 \amp 0 \\ -2 \amp 3 \amp 7\end{bmatrix} + 3\det\begin{bmatrix}1 \amp 0 \amp 1 \\ 5 \amp 0 \amp 0 \\ -2 \amp 1 \amp 7\end{bmatrix} -4\det\begin{bmatrix}1 \amp 0 \amp 2 \\ 5 \amp 0 \amp 4 \\ -2 \amp 1 \amp 3\end{bmatrix}</me>.
			<p>On the other hand, if we do cofactor expansion along the second column (and remember the sign convention described above!) then we get:</p>
			<md>
				<mrow> \det(A) \amp = -2\det\begin{bmatrix}1 \amp 2 \amp 1 \\ 5 \amp 4 \amp 4 \\ -2 \amp 3 \amp 7\end{bmatrix} + 0\det\begin{bmatrix}1 \amp 3 \amp 4 \\ 5 \amp 4 \amp 4 \\ -2 \amp 3 \amp 7\end{bmatrix} -0\det\begin{bmatrix}1 \amp 3 \amp 4 \\ 1 \amp 2 \amp 1 \\ -2 \amp 3 \amp 7\end{bmatrix} + 1\det\begin{bmatrix}1 \amp 3 \amp 4 \\ 1 \amp 2 \amp 1 \\ 5 \amp 4 \amp 4\end{bmatrix} </mrow>
				<mrow> \amp = -2\det\begin{bmatrix}1 \amp 2 \amp 1 \\ 5 \amp 4 \amp 4 \\ -2 \amp 3 \amp 7\end{bmatrix} + 1\det\begin{bmatrix}1 \amp 3 \amp 4 \\ 1 \amp 2 \amp 1 \\ 5 \amp 4 \amp 4\end{bmatrix} </mrow>
			</md>.
			<p> While neither of these calculations are particularly appealing, the second one is certainly much shorter, as it only requires us to compute two <m>3 \times 3</m> determinants.  Regardless of which method we choose we will get the same answer, <m>\det(A) = 77</m>. </p>
		</example>

		<definition xml:id="def-triangular">
			<p> An <m>n \times n</m> matrix <m>A</m> is called a <em>lower triangular</em> matrix if every entry above the main diagonal is <m>0</m> (that is, if <m>a_{i,j} = 0</m> whenever <m>i \lt j</m>).  It is called an <em>upper triangular</em> matrix if every entry below the main diagonal is <m>0</m> (that is, if <m>a_{i,j} = 0</m> whenever <m>i > j</m>).  In either case the matrix may also have <m>0</m> entries elsewhere.</p>
			<p> A matrix is <em>triangular</em> if it is either upper triangular or lower triangular. </p>
		</definition>

		<example xml:id="ex-triangular-determinant">
			<p>Let <m>A = \begin{bmatrix}1 \amp 2 \amp 3 \amp 4 \\ 0 \amp -2 \amp 1 \amp 0 \\ 0 \amp 0 \amp 3 \amp 5 \\ 0 \amp 0 \amp 0 \amp -3\end{bmatrix}</m>.  Then <m>A</m> is upper triangular.</p>
			<p>If we use cofactor expansion along the first column, and then continue using the first column in subsequent determinants, we obtain:
			<md>
				<mrow>\det(A) \amp = 1\det\begin{bmatrix}-2 \amp 1 \amp 0 \\ 0 \amp 3 \amp 5 \\ 0 \amp 0 \amp -3\end{bmatrix} </mrow>
				<mrow> \amp = 1(-2)\det\begin{bmatrix}3 \amp 5 \\ 0 \amp -3\end{bmatrix} </mrow>
				<mrow> \amp = 1(-2)(3)(-3) </mrow>
			</md>.</p>
		</example>

		<fact xml:id="fact-triangular-determinant">
			<p> What we saw in <xref ref="ex-triangular-determinant" /> always happens for triangular matrices: <em>If <m>A</m> is triangular</em>, then <m>\det(A)</m> is the product of the diagonal entries of <m>A</m>.</p>
		</fact>
	</subsection>

	<subsection>
		<title> Properties of determinants </title>

		<p>It is time to consider some properties of the determinant and how it interacts with other operations we perform on matrices.  After seeing <xref ref="ex-triangular-determinant" /> you might hope that we could row-reduce our matrix to a triangular form (such as the reduced row echelon form) before computing the determinant.  We can, but the row operations change the determinant, so we need to keep track of our row operations and how they affect the determinant.</p>
		<theorem xml:id="thm-row-ops-det">
			<statement>
				<p> Suppose that <m>A</m> is an <m>n \times n</m> matrix, and <m>B</m> is obtained from <m>A</m> by a single elementary row operation.  Then:
				<ol>
					<li> If <m>A \to_{R_i \leftrightarrow R_j} B</m> then <m>\det(A) = -\det(B)</m>.</li>
					<li> If <m>A \to_{kR_i} B</m> then <m>\det(A) = \frac{1}{k}\det(B)</m>.</li>
					<li> If <m>A \to_{R_i+cR_j} B</m> then <m>\det(A) = \det(B)</m>.</li>
				</ol>
				</p>
			</statement>
		</theorem>

		<example>
			<p> Let <m>A = \begin{bmatrix}2 \amp 3 \amp 0 \\ 3 \amp 1 \amp 1 \\ 4 \amp 2 \amp -1\end{bmatrix}</m>.  We row-reduce <m>A</m>, keeping track of our row operations.  To help clarify the determinant calculation at the end, we will give each intermediate matrix a name (one does not typically do this, it is just for exposition purposes). </p>
			<md>
				<mrow>A \amp \to_{\frac{1}{2}R_1} \begin{bmatrix}1 \amp 3/2 \amp 0 \\ 3 \amp 1 \amp 1 \\ 4 \amp 2 \amp -1\end{bmatrix} \amp = B_1</mrow>
				<mrow>\amp \to_{R_2-3R_1} \begin{bmatrix}1 \amp 3/2 \amp 0 \\ 0 \amp -7/2 \amp 1 \\ 4 \amp 2 \amp 1\end{bmatrix} \amp = B_2</mrow>
				<mrow>\amp \to_{R_3 - 4R_1}\begin{bmatrix}1 \amp 3/2 \amp 0 \\ 0 \amp -7/2 \amp 1 \\ 0 \amp -4 \amp -1\end{bmatrix} \amp = B_3</mrow>
				<mrow>\amp \to_{R_2 \leftrightarrow R_3}\begin{bmatrix}1 \amp 3/2 \amp 0 \\ 0 \amp -4 \amp -1 \\ -7/2 \amp 1 \amp 0\end{bmatrix} \amp = B_4</mrow>
				<mrow>\amp \to_{-\frac{1}{4}R_2}\begin{bmatrix}1 \amp 3/2 \amp 0 \\ 0 \amp 1 \amp 1/4 \\ -7/2 \amp 1 \amp 0\end{bmatrix}\amp = B_5 </mrow>
				<mrow>\amp \to_{2R_3}\begin{bmatrix}1 \amp 3/2 \amp 0 \\ 0 \amp 1 \amp 1/4 \\ 0 \amp -7 \amp 2\end{bmatrix} \amp = B_6</mrow>
				<mrow>\amp \to_{R_3+7R_2}\begin{bmatrix}1 \amp 3/2 \amp 0 \\ 0 \amp 1 \amp 1/4 \\ 0 \amp 0 \amp 15/4\end{bmatrix} \amp = B_7</mrow>
			</md>
			<p> Now we've reached a triangular matrix, so the determinant of <m>B_7</m> is the product of its diagonal entries, <m>\det(B_7) = 15/4</m>.  To find the determinant of <m>A</m> we work backwards, using <xref ref="thm-row-ops-det" />.</p>
			<md>
				<mrow> \det(A) \amp = 2\det(B_1)</mrow>
				<mrow> \amp = 2\det(B_2)</mrow>
				<mrow> \amp = 2\det(B_3)</mrow>
				<mrow> \amp = 2(-\det(B_4))</mrow>
				<mrow>\amp = -2\det(B_4) </mrow>
				<mrow> \amp = -2(-4)\det(B_5) </mrow>
				<mrow> \amp = 8\det(B_5)</mrow>
				<mrow> \amp = 8\left(\frac{1}{2}\right)\det(B_6) </mrow>
				<mrow> \amp = 4\det(B_6)</mrow>
				<mrow> \amp = 4\det(B_7) </mrow>
				<mrow> \amp = 4\left(\frac{15}{4}\right)</mrow>
				<mrow> \amp = 15 </mrow>
			</md>
			<p> Thus <m>\det(A) = 15</m>.  You can verify this by computing <m>\det(A)</m> directly using cofactor expansion. </p>
		</example>
		<p> As you can see from the example, row reduction can somewhat simplify the process of calculating determinants, but it remains a tedious process.  In fact, in real-life applications of linear algebra (where matrices are often very large), calculating determinants simply isn't practical - it is too time-consuming and error-prone, even when done on a computer.  Nevertheless, for certain problems the determinant can be very helpful.  For our purposes, the most important property of determinants is that they provide a numerical way to detect invertibility of matrices; specifically, a square matrix is invertible if and only if it has non-zero determinant.  We will make extensive use of this method for detecting invertibility in <xref ref="sec-Eigen" />.  For now, we get to add another statement to the Fundamental Theorem!</p>
		<theorem xml:id="thm-fundamental-determinant">
			<title>Fundamental Theorem - Version 3</title>
			<statement>
				<p> Let <m>A</m> be an <m>n \times n</m> matrix.  The following are equivalent: </p>
				<ol>
					<li> <m>\RREF(A) = I_n</m>.</li>
					<li> <m>A</m> is invertible. </li>
					<li> The system <m>[A|\vec{0}]</m> has a unique solution. </li>
					<li> The equation <m>A\vec{x} = \vec{0}</m> has a unique solution. </li>
					<li> For every vector <m>\vec{b}</m> in <m>\mathbb{R}^n</m>, the system <m>[A|\vec{b}]</m> has a unique solution.</li>
					<li> For every vector <m>\vec{b}</m> in <m>\mathbb{R}^n</m>, the equation <m>A\vec{x} = \vec{b}</m> has a unique solution. </li>
					<li> The columns of <m>A</m> are linearly independent. </li>
					<li> The span of the columns of <m>A</m> is <m>\mathbb{R}^n</m>. </li>
					<li> <m>\rank(A) = n</m>.</li>
					<li> <m>A</m> can be written as a product of a finite collection of elementary matrices. </li>
					<li> <m>\det(A) \neq 0</m>.</li>
				</ol>
			</statement>
			<proof>
				<p> The key observation for both parts of this proof is that it follows from <xref ref="thm-row-ops-det" /> that, although row operations can change determinants, they never change whether or not the determiant is zero. </p>
				<p> <m>1 \implies 11</m>: Suppose that <m>\RREF(A) = I_n</m>.  Then there is a sequence of row operations that transforms <m>A</m> into <m>I_n</m>, and <m>\det(I_n) \neq 0</m>, so <m>\det(A) \neq 0</m>. </p>
				<p> <m>11 \implies 1</m>: Suppose that <m>\det(A) \neq 0</m>.  Then <m>\det(\RREF(A)) \neq 0</m> as well.  In particular, <m>\RREF(A)</m> does not have a row of all zero, so every row of <m>\RREF(A)</m> has a leading <m>1</m>.  Thus <m>A</m> has <m>n</m> pivot columns.  The only <m>n \times n</m> matrix in reduced row echelon form with <m>n</m> pivot columns is <m>I_n</m>, so <m>\RREF(A) = I_n</m>.</p>
			</proof>
		</theorem>

		<p> To wrap up this part of our discussion, here is how determinants interact with the other matrix operations we have seen. </p>
		<theorem xml:id="thm-matrix-operations-det">
			<statement><p> Suppose that <m>A</m> and <m>B</m> are <m>n \times n</m> matrices.  Then: 
				<ol>
					<li> For any scalar <m>k</m>, <m>\det(kA) = k^n\det(A)</m>. </li>
					<li> <m>\det(AB) = \det(A)\det(B)</m>. </li>
					<li> If <m>A</m> is invertible then <m>\det(A^{-1}) = \frac{1}{\det(A)}</m>.</li>
					<li> <m>\det(A^t) = \det(A)</m>. </li>
				</ol>
			</p></statement>
		</theorem>
		<note><p> 
			Conspicuously absent from the theorem above is a nice formula for computing <m>\det(A+B)</m> from <m>\det(A)</m> and <m>\det(B)</m>.  The reason for that omission is that no such formula exists: If all that you know about <m>A</m> and <m>B</m> is their determinants then you do not have enough information to compute <m>\det(A+B)</m>.
		</p></note>
	</subsection>


	<subsection xml:id="subsec-geometric-determinant">
		<title> A geometric interpretation </title>
		<p> Recall that every <m>n \times n</m> matrix is associated with a linear transformation <m>T_A : \mathbb{R}^n \to \mathbb{R}^n</m>, where <m>T_A(\vec{v}) = A\vec{v}</m>.  The number <m>\det(A)</m> has a geometric meaning in terms of the transformation <m>T_A</m>.  The geometry is easiest to visualize in the case of <m>2 \times 2</m> matrices, so we start there.</p>
		<p> Suppose that <m>A = \begin{bmatrix}a \amp b \\ c \amp d\end{bmatrix}</m>.  Then <m>T_A\left(\begin{bmatrix}1\\0\end{bmatrix}\right) = \begin{bmatrix}a\\c\end{bmatrix}</m> and <m>T_A\left(\begin{bmatrix}b\\d\end{bmatrix}\right)</m>.  Any point inside the square determined by <m>\begin{bmatrix}1\\0\end{bmatrix}</m> and <m>\begin{bmatrix}0\\1\end{bmatrix}</m> can be written as <m>s\begin{bmatrix}1\\0\end{bmatrix}+t\begin{bmatrix}0\\1\end{bmatrix}</m>, where <m>0 \leq s \leq 1</m> and <m>0 \leq t \leq 1</m>.  Since <m>T_A</m> is a linear transformation, we have <me> T_A\left(s\begin{bmatrix}1\\0\end{bmatrix}+t\begin{bmatrix}0\\1\end{bmatrix}\right) = sT_A\left(\begin{bmatrix}1\\0\end{bmatrix}\right) + tT_A\left(\begin{bmatrix}0\\1\end{bmatrix}\right) = s\begin{bmatrix}a\\c\end{bmatrix}+t\begin{bmatrix}b\\d\end{bmatrix}</me>.  That is, <em>points inside the square determined by <m>\begin{bmatrix}1\\0\end{bmatrix}</m> and <m>\begin{bmatrix}0\\1\end{bmatrix}</m> are sent to points inside the parallelogram determined by <m>\begin{bmatrix}a\\c\end{bmatrix}</m> and <m>\begin{bmatrix}b\\d\end{bmatrix}</m></em>.</p>
			<figure>
				<caption>The action of <m>T_A</m> on the unit square determined by <m>\vec{e_1}</m> and <m>\vec{e_2}</m>.</caption>
				<image>
					<latex-image>
						\begin{tikzpicture}	
							\draw[thick] (0,-2.2) -- (0,2.2);
							\draw[thick] (-2.2,0) -- (2.2,0);
							\draw[thick] (4.2,0) -- (8.2, 0);
							\draw[thick] (6.1, -2.2) -- (6.1, 2.2);
							
							\draw[pattern=north west lines, pattern color=purple] (0, 0)--(0,1.5)--(1.5,1.5)--(1.5,0)--cycle;
							\draw[-Triangle, blue, ultra thick] (0, 0) -- (1.5, 0) node[below]{$\vec{e_1}$};
							\draw[-Triangle, red, ultra thick] (0, 0) -- (0, 1.5) node[left]{$\vec{e_2}$};
							
							\draw[pattern=north west lines, pattern color=purple] (6.1, 0)--(7,0.5)--(7.5, 2)--(6.6,1.5)--cycle;
							\draw[-Triangle, blue, ultra thick] (6.1, 0)--(7,0.5) node[below right]{$T_A(\vec{e_1})$};
							\draw[-Triangle, red, ultra thick] (6.1, 0)--(6.6, 1.5) node[pos=0.5, left]{$T_A(\vec{e_2})$};
							
							\draw[Triangle-] (4,2) arc (20:160:.8);
							\node at (3.2, 3){$T_A$};

						\end{tikzpicture}
					</latex-image>
				</image>
			</figure>
		<p> In the setting described above, the geometric interpretation of the determinant is that the parallelogram determined by <m>T_A(\vec{e_1})</m> and <m>T_A(\vec{e_2})</m> (that is, determined by the columns of <m>A</m>) has area exactly <m>\abs{\det(A)}</m>.  The sign of <m>\det(A)</m> is determined by whether <m>T_A</m> preserves or reverses the orientation between the sides of the square.  The figure above shows the situation for a positive determinant.  Here is what the image looks like if the determinant is negative - notice how the positions of <m>T_A(\vec{e_1})</m> and <m>T_A(\vec{e_2})</m> are reversed from those of <m>\vec{e_1}</m> and <m>\vec{e_2}</m>.</p>
			<figure>
				<caption>A negative determinant.</caption>
				<image>
					<latex-image>
						\begin{tikzpicture}	
							\draw[thick] (0,-2.2) -- (0,2.2);
							\draw[thick] (-2.2,0) -- (2.2,0);
							\draw[thick] (4.2,0) -- (8.2, 0);
							\draw[thick] (6.1, -2.2) -- (6.1, 2.2);
							
							\draw[pattern=north west lines, pattern color=purple] (0, 0)--(0,1.5)--(1.5,1.5)--(1.5,0)--cycle;
							\draw[-Triangle, blue, ultra thick] (0, 0) -- (1.5, 0) node[below]{$\vec{e_1}$};
							\draw[-Triangle, red, ultra thick] (0, 0) -- (0, 1.5) node[left]{$\vec{e_2}$};
							
							\draw[pattern=north west lines, pattern color=purple] (6.1, 0)--(7,0.5)--(7.5, 2)--(6.6,1.5)--cycle;
							\draw[-Triangle, red, ultra thick] (6.1, 0)--(7,0.5) node[below right]{$T_A(\vec{e_2})$};
							\draw[-Triangle, blue, ultra thick] (6.1, 0)--(6.6, 1.5) node[pos=0.5, left]{$T_A(\vec{e_1})$};
							
							\draw[Triangle-] (4,2) arc (20:160:.8);
							\node at (3.2, 3){$T_A$};

						\end{tikzpicture}
					</latex-image>
				</image>
			</figure>
			<p> In higher dimensions the geometry is similar: <m>\abs{\det(A)}</m> is the volume of the image of the parallelopiped determined by the standard basis of <m>\mathbb{R}^n</m>, and the sign of <m>\det(A)</m> is determined by the the orientation of the image. </p>
			<p> In multivariable calculus you will see that determinants appear in the formula for change-of-variables in multiple integrals.  The reason is fundamentally because of the geometric picture shown above - the determinant is giving the correction factor for how areas (or volumes, or their higher-dimensional analogues) have changed in applying a linear transformation to change coordinates.  In fact, this use of determinants was discovered before the modern notions of "matrix" and "linear transformation"!</p>
	</subsection>

	<xi:include href="3-5-exercises.ptx" />
</section>