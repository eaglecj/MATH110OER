<section xml:id="sec-Diagonalization" xmlns:xi="http://www.w3.org/2001/XInclude">

	<title>
		Diagonalization
	</title>
	<subsection>
		<title>
			Similarity
		</title>

		<definition>
			<p> Let <m>A</m> and <m>B</m> be <m>n \times n</m> matrices.  We say that <m>A</m> is <em>similar to <m>B</m></em> if there is an invertible <m>n \times n</m> matrix <m>P</m> such that <m>A = PBP^{-1}</m>.  In this case we write <m>A \sim B</m> to indicate that <m>A</m> is similar to <m>B</m>. </p>
		</definition>
		<note><p>Caution! The word "similar" has an ordinary English meaning, and now it also has a technical meaning.  When we say <m>A</m> is similar to <m>B</m> we always mean the precise statement defined above, not the ordinary English meaning.</p></note>
		<p> Here are some basic facts about the relation <m>\sim</m>. </p>
		<theorem>
			<statement>
				<p> Suppose that <m>A</m>, <m>B</m>, and <m>C</m> are <m>n \times n</m> matrices.  Then:</p>
				<ol>
					<li> <m>A \sim A</m>. </li>
					<li> If <m>A \sim B</m> then <m>B \sim A</m>. </li>
					<li> If <m>A \sim B</m> and <m>B \sim C</m> then <m>A \sim C</m>.</li>
				</ol>
			</statement>
			<proof>
				<p> We will only prove the second statement, leaving the others as exercises.  Suppose that <m>A \sim B</m>.  Then there is an invertible matrix <m>P</m> such that <m>A = PBP^{-1}</m>.  Multiplying on the left by <m>P^{-1}</m> and on the right by <m>P</m> we get <m>P^{-1}AP = B</m>.  Now let <m>Q = P^{-1}</m>.  Then <m>Q</m> is an invertible matrix and <m>B = QAQ^{-1}</m>, so <m>B \sim A</m>.</p>
			</proof>
		</theorem>
		<p> In light of point (2) above, we sometimes say that <em><m>A</m> and <m>B</m> are similar</em> rather than saying <m>A</m> is similar to <m>B</m>, since the order doesn't actually matter.  As you would expect from the name, similar matrices share many properties.  Here are a few examples of those propertes. </p>
		<theorem xml:id="thm-similar-same-char-poly">
			<statement>
				<p> Suppose that <m>A</m> and <m>B</m> are similar <m>n \times n</m> matrices.  Then <m>A</m> and <m>B</m> have the same characteristic polynomial, and therefore also have the same eigenvalues with the same algebraic multiplicities.</p>
			</statement>
			<proof>
				<p> Suppose that <m>A = PBP^{-1}</m>.  Notice that for any scalar <m>x</m> we have <me>P(xI_n)P^{-1} = x(PI_nP^{-1}) = xPP^{-1} = xI_n</me>.  This, together with properties of the determinant that we saw in <xref ref="sec-Determinants" />, allows us to calculate as follows: </p>
				<md>
					<mrow>\chi_A(x) \amp = \det(A - xI_n)</mrow>
					<mrow>\amp = \det(PBP^{-1} - xI_n)</mrow>
					<mrow>\amp = \det(PBP^{-1} - P(xI_n)P^{-1})</mrow>
					<mrow>\amp = \det(P(B - xI_n)P^{-1})</mrow>
					<mrow>\amp = \det(P)\det(B-xI_n)\det(P^{-1})</mrow>
					<mrow>\amp = \det(P)\det(B-xI_n)\frac{1}{\det(P)}</mrow>
					<mrow>\amp = \det(B-xI_n) </mrow>
					<mrow>\amp = \chi_B(x)</mrow>
				</md>
			</proof>
		</theorem>
		<corollary>
			<statement>
				<p> If <m>A</m> and <m>B</m> are similar matrices then <m>\det(A) = \det(B)</m>.</p>
			</statement>
			<proof>
				<p> Using <xref ref="thm-similar-same-char-poly" /> applied when <m>x=0</m> we get:
				<me>\det(A) = \det(A - 0I_n) = \chi_A(0) = \chi_B(0) = \det(B - 0I_n) = \det(B)</me>.</p>
			</proof>
		</corollary>
		<p> We list some other properties shared by similar matrices, without proof. </p>
		<theorem>
			<statement>
				<p> Suppose that <m>A</m> and <m>B</m> are similar matrices.  Then:
				<ul>
					<li> For each eigenvalue <m>\lambda</m>, <m>\geo_A(\lambda) = \geo_B(\lambda)</m>.</li>
					<li><m>\rank(A) = \rank(B)</m>.</li>
					<li><m>\nullity(A) = \nullity(B)</m>.</li>
				</ul></p>
			</statement>
		</theorem>

		<note><p>Warning! When <m>A</m> and <m>B</m> are similar we have <m>\rank(A) = \rank(B)</m>, which means that <m>\RREF(A)</m> and <m>\RREF(B)</m> have the same number of non-zero rows.  It does <em>not</em> mean that <m>\RREF(A) = \RREF(B)</m>, and indeed often that isn't the case.  Similarly (pardon the pun), although for each eigenvalue <m>\lambda</m> we have <m>\dim(E_A(\lambda)) = \dim(E_B(\lambda))</m>, it is often <em>not</em> true that <m>E_A(\lambda) = E_B(\lambda)</m>.</p></note>
	</subsection>

	<subsection>
		<title>
			Diagonalization
		</title>
		<p> As we have seen, if <m>A \sim B</m> then <m>A</m> and <m>B</m> have many properties in common.  The easiest matrices to study are the diagonal matrices, so given a matrix <m>A</m> we would like to know if there is a diagonal matrix <m>D</m> such that <m>A \sim D</m>.  In this section we will find out when that happens, and see some examples of how it can help us. </p>
		<definition>
			<p> Let <m>A</m> be a square matrix.  We say that <m>A</m> is <em>diagonalizable</em> if there is a diagonal matrix <m>D</m> such that <m>A \sim D</m>.</p>
		</definition>

		<example xml:id="ex-not-diagonalizable">
			<p> Let <m>A = \begin{bmatrix}-3 \amp 1 \\ 0 \amp -2\end{bmatrix}</m>.  We will show that <m>A</m> is not diagonalizable.  We could try to do this by hand: Let <m>P = \begin{bmatrix}a \amp b \\ c \amp d\end{bmatrix}</m> and <m>D = \begin{bmatrix}x \amp 0 \\ 0 \amp y\end{bmatrix}</m>, and then show directly that the equation <m>\begin{bmatrix}A = PDP^{-1}</m> cannot be solved.  While this is possible, it is rather unpleasant.</p>
			<p> A much easier way is to calculate that the only eigenvalue of <m>A</m> is <m>-3</m>, which occurs with algebraic multiplicity <m>2</m>.  By <xref ref="thm-similar-same-char-poly" /> if <m>A \sim D</m> then <m>D</m> must also have only <m>-3</m> as an eigenvalue, and with algebraic multiplicity <m>2</m>.  The only such diagonal matrix is <m>D = \begin{bmatrix}-3 \amp 0 \\ 0 \amp -3\end{bmatrix}</m>, so if <m>A</m> is diagonalizable then we must have <m>A = P\begin{bmatrix}-3 \amp 0 \\ 0 \amp -3\end{bmatrix}P^{-1}</m>.  Then a little calculation gives: <me>A = P\begin{bmatrix}-3 \amp 0 \\ 0 \amp -3\end{bmatrix}P^{-1} = P(-3I)P^{-1} = (-3)PIP^{-1} = (-3)PP^{-1} = -3I = \begin{bmatrix}-3 \amp 0 \\ 0 \amp -3\end{bmatrix}</me>, but in fact that is not the matrix <m>A</m> we started with, so this shows that <m>A</m> is not diagonalizable.</p>
		</example>
		<example xml:id="ex-diagonalizable">
			<p> Let <m>A = \begin{bmatrix}2 \amp 0 \\ -5 \amp -3\end{bmatrix}</m>, <m>P = \begin{bmatrix}1 \amp 0 \\ -1 \amp 1\end{bmatrix}</m>, and <m>D = \begin{bmatrix}2 \amp 0 \\ 0 \amp -3\end{bmatrix}</m>.  Then <m>A = PDP^{-1}</m>, so <m>A</m> is diagonalizable.  By the end of this section we will see how to find the matrices <m>P</m> and <m>D</m> starting from <m>A</m>.</p>
		</example>

		<p>The method that we used in <xref ref="ex-not-diagonalizable" /> can be adapted to find the diagonal matrices that could be similar to a given matrix. </p>
		<theorem>
			<statement>
				<p> Let <m>A</m> be an <m>n \times n</m> matrix, and let <m>\lambda_1, \ldots, \lambda_n</m> be the eigenvalues of <m>A</m> (repeated according to their algebraic multiplicities).  If <m>A \sim D</m> and <m>D</m> is diagonal then the diagonal entries of <m>D</m> are <m>\lambda_1, \ldots, \lambda_n</m> in some order. </p>
			</statement>
			<proof>
				<p> If <m>D</m> has diagonal entries <m>d_1, \ldots, d_n</m> then by <xref ref="thm-eigenvalues-triangular" /> the eigenvalues of <m>D</m>, listed according to their algebraic multiplicities, are <m>d_1, \ldots, d_n</m>.  If <m>A\sim D</m> then by <xref ref="thm-similar-same-char-poly" /> the eigenvalues of <m>D</m>, listed according to their algebraic multiplicities, are <m>\lambda_1, \ldots, \lambda_n</m>.  So the lists <m>d_1, \ldots, d_n</m> and <m>\lambda_1, \ldots, \lambda_n</m> must be the same except for possibly the order in which they are written. </p>
			</proof>
		</theorem>

		<p>The theorem above tells us which diagonal matrices <m>A</m> <em>might</em> be similar to, but (as seen in <xref ref="ex-not-diagonalizable" />) it does not guarantee that <m>A</m> is actually similar to a diagonal matrix.  The missing ingredient is the matrix <m>P</m> from the definition of similarity.  An example will help to explain where <m>P</m> comes from, and then we will give the general theorem.</p>

		<example>
			<p> Consider again the matrices <m>A = \begin{bmatrix}2 \amp 0 \\ -5 \amp -3\end{bmatrix}</m>, <m>P = \begin{bmatrix}1 \amp 0 \\ -1 \amp 1\end{bmatrix}</m>, and <m>D = \begin{bmatrix}2 \amp 0 \\ 0 \amp -3\end{bmatrix}</m> from <xref ref="ex-diagonalizable" />.  In that example we had <m>A = PDP^{-1}</m>, which is equivalent to <m>AP = PD</m>.</p>
			<p>Using the definition of matrix multiplication, the first column of <m>AP</m> is <me>\begin{bmatrix}2 \amp 0 \\ -5 \amp -3\end{bmatrix}\begin{bmatrix}\begin{bmatrix}1 \\ -1\end{bmatrix} = \begin{bmatrix}2 \\ -2\end{bmatrix} = 2\begin{bmatrix}1\\-1\end{bmatrix}</me> and the second column is <me>\begin{bmatrix}2 \amp 0 \\ -5 \amp -3 \end{bmatrix}\begin{bmatrix}0\\1\end{bmatrix} = \begin{bmatrix}0\\-3\end{bmatrix} = -3\begin{bmatrix}0\\-3\end{bmatrix}</me>.  We see that the columns of <m>P</m> are eigenvectors of <m>A</m>, and even more, that the eigenvalue for the first column of <m>P</m> is the first diagonal entry of <m>D</m> and the eigenvalue for the second column of <m>P</m> is the second diagonal entry of <m>D</m>. </p>
		</example>
		<theorem>
			<statement>
				<p>Let <m>A</m> be an <m>n \times n</m> matrix, and suppose that <m>A = PDP^{-1}</m> where <m>D</m> is a diagonal matrix.  Then the columns of <m>P</m> are eigenvectors of <m>A</m>, and for each <m>j</m> the eigenvalue associated to the <m>j</m>th column of <m>P</m> is the <m>(j,j)</m> entry of <m>D</m>.</p>
			</statement>
			<proof>
				<p> Let <m>P</m> have columns <m>\vec{P_1}, \ldots, \vec{P_n}</m>, and suppose that <m>D = \begin{bmatrix}\lambda_1 \amp 0 \amp \cdots \amp 0 \\ 0 \amp \lambda_2 \amp \cdots \amp 0 \\ \vdots \amp \vdots \amp \ddots \amp \vdots \\ 0 \amp 0 \amp \cdots \amp \lambda_n\end{bmatrix}</m>.  Rearrange the equation <m>A = PDP^{-1}</m> to <m>AP = PD</m>.  Now using the definition of matrix multiplication, for any <m>j</m> the <m>j</m>th column on the left is <m>A\vec{P_j}</m> and the <m>j</m>th column on the right is <m>P\begin{bmatrix}0 \\ \vdots \\ 0 \\ \lambda_j \\ 0 \\ \vdots \\ 0\end{bmatrix} = \lambda_j\vec{P_j}</m>.  We therefore have <m>A\vec{P_j} = \lambda_jP_j</m>, as required.</p>
			</proof>
		</theorem>

		<p>Now we have all the information we need to find <m>P</m> and <m>D</m>, assuming that our matrix <m>A</m> is diagonalizable.  But we know that not every matrix is diagonalizable (<xref ref="ex-not-diagonalizable" />), so what went wrong? </p>
		<p>As long as we allow our eigenvalues to come from <m>\mathbb{C}</m>, there is no problem constructing <m>D</m>, because <xref ref="fact-enough-eigenvalues" /> guarantees that an <m>n \times n</m> matrix has exactly <m>n</m> eigenvalues (counted according to algebraic multiplicity).  So the failure must be because of something going wrong in building <m>P</m>.</p>
		<p>If we are to have a chance of writing <m>A = PDP^{-1}</m> we need the columns of <m>P</m> to be eigenvectors of <m>A</m>, but we also need <m>P</m> to be invertible.  Using the <xref ref="thm-fundamental-with-eigenvalues" text="custom">Fundamental Theorem</xref> we can restate "<m>P</m> is invertible" as "<m>P</m> has linearly independent columns".  So the problem we could encounter is that perhaps there are not enough linearly independent eigenvectors to form an invertible matrix <m>P</m>.  In fact, that is the only possible obstacle; while we haven't formally proved it (and nor will we), hopefully you now find the following theorem believable.</p>

		<theorem>
			<statement>
				<p>Let <m>A</m> be an <m>n \times n</m> matrix.  Then <m>A</m> is diagonalizable if and only if it is possible to find <m>n</m> linearly independent eigenvectors for <m>A</m>.</p>
			</statement>
		</theorem>

		<p> We already know from <xref ref="thm-indep-eigenvectors" /> that eigenvectors from different eigenvalues are linearly independent.  With some more work (which we won't do here) one can prove that this implies that our search for <m>n</m> independent eigenvectors is really a search for <m>\alg_A(\lambda)</m> many independent eigenvectors for each eigenvalue <m>\lambda</m>.  We know that the maximum number of linearly independent <m>\lambda</m>-eigenvectors of <m>A</m> is <m>\geo_A(\lambda) = \dim(E_A(\lambda))</m>, so we finally come to the main result about diagonalizability. </p>

		<theorem xml:id="thm-diagonalization-main">
			<statement>
				<p> Let <m>A</m> be an <m>n \times n</m> matrix.  Then <m>A</m> is diagonalizable if and only if for every eigenvalue <m>\lambda</m> of <m>A</m> we have <m>\geo_A(\lambda) = \alg_A(\lambda)</m>.</p><p>In that case <m>A = PDP^{-1}</m> where the diagonal entries of <m>D</m> are the eigenvalues of <m>A</m> repeated according to their algebraic multiplicities, and the columns of <m>P</m> are linearly independent eigenvectors of <m>A</m> arranged in an order so that the <m>j</m>th column of <m>P</m> is an eigenvector for the <m>(j,j)</m> entry of <m>D</m>.</p>
			</statement>
		</theorem>

		<example>
			<p> Let <m>A = \begin{bmatrix}0 \amp 1 \amp 3 \\ 2 \amp -1 \amp 3 \\ -2 \amp 5 \amp 1\end{bmatrix}</m>.  Determine whether or not <m>A</m> is diagonalizable, and if it is, write it in the form <m>A = PDP^{-1}</m> with <m>D</m> a diagonal matrix.</p>
			<solution>
				<p> We start by finding the eigenvalues of <m>A</m>.
				<me>\chi_A(x) = \det(A - xI_3) = \det\begin{bmatrix}-x \amp 1 \amp 3 \\ 2 \amp -1-x \amp 3 \\ -2 \amp 5 \amp 1-x\end{bmatrix} = -x^3 +12x +16 = -(x-4)(x+2)^2</me>.</p>
				<p>Now we know that the eigenvalues are <m>4</m>, with algebraic multiplicitiy <m>1</m>, and <m>-2</m>, with algebraic multiplicity <m>2</m>.</p>
				<p>Next we need to find out the geometric multiplicities of the eigenvalues, to see if we have enough linearly independent eigenvectors.  Let's start with the eigenvalue <m>-2</m>.  To find a basis for the eigenspace, we row-reduce:
				<me>A-(-2)I_3 = \begin{bmatrix}2 \amp 1 \amp 3 \\ 2 \amp 1 \amp 3 \\ -2 \amp 5 \amp 3\end{bmatrix} \to \begin{bmatrix}1 \amp 0 \amp 1 \\ 0 \amp 1 \amp 1 \\ 0 \amp 0 \amp 0\end{bmatrix}</me>.</p>
				<p>From here we see that vectors in <m>E_A(-2)</m> have the form <m>\begin{bmatrix}x\\y\\z\end{bmatrix} = \begin{bmatrix}-z\\-z\\z\end{bmatrix} = z\begin{bmatrix}-1\\-1\\1\end{bmatrix}</m>.  Therefore <m>\left\{\begin{bmatrix}-1\\-1\\1\end{bmatrix}\right\}</m> is a basis for <m>E_A(-2)</m>, and so <m>\geo_A(-2) = 1</m>.  We thus have <m>\geo_A(-2) = 1 \lt 2 = \alg_A(-2)</m>, so by <xref ref="thm-diagonalization-main" /> the matrix <m>A</m> is not diagonalizable.</p>
				<p>Notice that since we already found one eigenvalue with <m>\alg_A(\lambda) \neq \geo_A(\lambda)</m> there is no reason for us to consider any other eigenvalues.</p>
			</solution>
		</example>

		<example xml:id="ex-diagonalizable-2">
			<p> Let <m>A = \begin{bmatrix}1 \amp 2 \amp 0 \\ -1 \amp 4 \amp 0 \\ 1 \amp -1 \amp 3\end{bmatrix}</m>.  Determine whether or not <m>A</m> is diagonalizable, and if it is, write it in the form <m>A = PDP^{-1}</m> with <m>D</m> a diagonal matrix.</p>
			<solution>
				<p>The first step is again to find the eigenvalues of <m>A</m>.
				<me>\chi_A(x) = \det(A-xI_3) = \det\begin{bmatrix}1-x \amp 2 \amp 0 \\ -1 \amp 4-x \amp 0 \\ 1 \amp -1 \amp 3-x\end{bmatrix} = -x^3+8x^2-21x+18 = -(x-2)(x-3)^2</me>.</p>
				<p>The eigenvalues are <m>2</m>, with algebraic multiplicity <m>1</m>, and <m>3</m>, with algebraic multiplicity <m>2</m>.</p>
				<p>We next find a basis for <m>E_A(3)</m>, by row-reducing:
				<me>A-3I_3 = \begin{bmatrix}-2 \amp 2 \amp 0 \\-1 \amp 1 \amp 0 \\ 1 \amp -1 \amp 0\end{bmatrix} \to \begin{bmatrix}1 \amp -1 \amp 0 \\ 0 \amp 0 \amp 0 \\ 0 \amp 0 \amp 0\end{bmatrix}</me>.</p>
				<p>We see that vectors in <m>E_A(3)</m> look like <m>\begin{bmatrix}x\\y\\z\end{bmatrix} = \begin{bmatrix}y\\y\\z\end{bmatrix} = y\begin{bmatrix}1\\1\\0\end{bmatrix} + z\begin{bmatrix}0\\0\\1\end{bmatrix}</m>.  Therefore <m>\left\{\begin{bmatrix}1\\1\\0\end{bmatrix}, \begin{bmatrix}0\\0\\1\end{bmatrix}\right\}</m> is a basis for <m>E_A(3)</m>.  In particular, <m>\geo_A(3) = 2</m>.  This matches with <m>\alg_A(3) = 2</m>, so we move on to the next eigenvalue.</p>
				<p>We look for a baiss for <m>E_A(2)</m>:
				<me>A-2I_3 = \begin{bmatrix}-1 \amp 2 \amp 0 \\ -1 \amp 2 \amp 0 \\ 1 \amp -1 \amp 1\end{bmatrix} \to \begin{bmatrix}1 \amp 0 \amp 2 \\ 0 \amp 1 \amp 1 \\ 0 \amp 0 \amp 0\end{bmatrix}</me>.</p>
				<p>Therefore vectors in <m>E_A(2)</m> have the form <m>\begin{bmatrix}x\\y\\z\end{bmatrix} = \begin{bmatrix}-2z \\ -z \\ z\end{bmatrix} = z\begin{bmatrix}-2\\-1\\1\end{bmatrix}</m>.  This shows that a basis for <m>E_A(2)</m> is <m>\left\{\begin{bmatrix}-2\\-1\\1\end{bmatrix}\right\}</m>.  It also shows that <m>\geo_A(2) = 1</m>, which matches with <m>\alg_A(2) = 1</m>.</p>
				<p>Since <m>\geo_A(\lambda) = \alg_A(\lambda)</m> for every eigenvalue <m>\lambda</m>, it follows from <xref ref="thm-diagonalization-main" /> that <m>A</m> is diagonalizable.</p>
				<p>To find the matrices <m>P</m> and <m>D</m>, we start by choosing an order to write the eigenvalues on the diagonal of <m>D</m>.  Any order is fine; we choose <m>D = \begin{bmatrix}3 \amp 0 \amp 0 \\ 0 \amp 2 \amp 0 \\ 0 \amp 0 \amp 3\end{bmatrix}</m>.  Next we need to make the columns of <m>P</m> to be eigenvectors in the corresponding order to the order we used for <m>D</m>.  Using the bases found above, we choose <m>P = \begin{bmatrix}1 \amp -2 \amp 0\\ 1 \amp -1 \amp 0\\ 0 \amp 1 \amp 1\end{bmatrix}</m>.  With these choices we have <m>A = PDP^{-1}</m> - that this equation is correct follows form <xref ref="thm-diagonalization-main" />, but you could check it by hand if you wish.</p>
			</solution>
		</example>

		<p>As the examples show, most of the time we need to do quite a bit of work to show that a matrix is diagonalizable, even with all the tools we now have at hand.  There is one situation where it is easy to detect diagonalizability. </p>
		<theorem>
			<statement>
				<p> If an <m>n \times n</m> matrix <m>A</m> has <m>n</m> distinct eigenvalues then <m>A</m> is diagonalizable.</p>
			</statement>
			<proof>
				<p> By <xref ref="fact-enough-eigenvalues" /> the algebraic multiplicities of the eigenvalues add up to <m>n</m>, so if there are <m>n</m> distinct eigenvalues then they must all have algebraic multiplicity <m>1</m>.  For each eigenvalue <m>\lambda</m> we therefore have, by <xref ref="thm-algebraic-geometric-multiplicity-inequality" />, <m>1 \leq \geo_A(\lambda) \leq \alg_A(\lambda) = 1</m>, which implies that <m>\geo_A(\lambda) = \alg_A(\lambda)</m>.  Therefore by <xref ref="thm-diagonalization-main" /> <m>A</m> is diagonalizable.</p>
			</proof>
		</theorem>
		<p> <xref ref="ex-diagonalizable-2" /> shows that an <m>n \times n</m> matrix can be diagonalizable without having <m>n</m> distinct eigenvalues, so the converse of the above theorem is not true.</p>
	</subsection>
	<subsection>
		<title>
			An application
		</title>
		<p>To conclude our discussion of diagonalization we give just one example of the usefulness of diagonalizing a matrix (there are many others!).  The example we give is a very small example of a <em>Markov chain</em>; Markov chains appear in modelling whenever the state of a system changes randomly in such a way that the probabilities involved depend only on the current state and no earlier ones.</p>
		<p> In the example we will use two key observation:
		<ol>
			<li> If <m>A = PDP^{-1}</m> then <me>A^2 = PDP^{-1}PDP^{-1} = PDIDP^{-1} = PD^2P^{-1}</me>, and more generally, for any <m>k \geq 1</m>, <m>A^k = PD^kP^{-1}</m>.</li>
			<li> If <m>D</m> is a diagonal matrix then so is <m>D^k</m>, and the diagonal entries of <m>D^k</m> are the <m>k</m>th powers of the diagonal entries of <m>D</m>.  Specifically, for any <m>k \geq 1</m>, <m>\begin{bmatrix}a \amp 0 \\ 0 \amp b\end{bmatrix}^k = \begin{bmatrix}a^k \amp 0 \\ 0 \amp b^k\end{bmatrix}</m>.</li>
		</ol>
		With these tools in mind, here is our example.</p>
		<example>
			<p> Consider a (fictional) student in a mathematics course.  At any given minute the student is either studying the course material or not studying the course material.  After meticulous observation it has been discovered that a student who is studying at one minute has a <m>60\%</m> chance of studying the next minute, while a student who is not studying at one minute has only a <m>10\%</m> chance of studying the next minute.  Suppose that you see a student studying.  What is the probability that they will still be studying one hour later? </p>
			<solution>
				<p> We model the situation using the <em>transition matrix</em> <m>A = \begin{bmatrix}0.6 \amp 0.1 \\ 0.4 \amp 0.9\end{bmatrix}</m>.  We think of the first coordinates of vectors as the probability of studying, and the second coordinate as the probability of not studying.  The key fact we need from probability is that if a given student has an <m>x\%</m> chance of studying at one minute then at the next minute their chance of studying is the top entry of <m>A\begin{bmatrix}x\\1-x\end{bmatrix}</m>.  Therefore the probability that the student is studying <m>2</m> minutes later is the top entry of <m>A\left(A\begin{bmatrix}x\\1-x\end{bmatrix}\right) = A^2\begin{bmatrix}x\\1-x\end{bmatrix}</m>, and in general the probability that the student is studying <m>k</m> minutes later is the top entry of <m>A^k\begin{bmatrix}x\\1-x\end{bmatrix}</m>.</p>
				<p>Our student is observed to be studying, so our starting state has <m>x = 1</m>.  We are thus interested in the top entry of <m>A^{60}\begin{bmatrix}1\\0\end{bmatrix}</m>.  We could multiply <m>A</m> by itself <m>60</m> times, but doing that would take quite a lot of calculation.  Instead, we diagonalize <m>A</m>.  Using the method of <xref ref="ex-diagonalizable-2" /> we find that <m>A = PDP^{-1}</m> where <m>P=\begin{bmatrix}-1 \amp 1\\ 1 \amp 4\end{bmatrix}</m> and <m>D = \begin{bmatrix}0.5 \amp 0 \\ 0 \amp 1\end{bmatrix}</m>.  We also calculate <m>P^{-1} = \begin{bmatrix}-0.8 \amp 0.2 \\ 0.2 \amp 0.2\end{bmatrix}</m> using the techniques for finding matrix inverses from <xref ref="sec-Inverses" />. Thus 
				<md>
					<mrow>A^{60} \amp = PD^{60}P^{-1}</mrow>
					<mrow> \amp = \begin{bmatrix}-1 \amp 1 \\ 1 \amp 4\end{bmatrix}\begin{bmatrix}0.5^{60} \amp 0 \\ 0 \amp 1^{60}\end{bmatrix}\begin{bmatrix}-0.8 \amp 0.2 \\ 0.2 \amp 0.2\end{bmatrix} </mrow>
					<mrow> \amp = \begin{bmatrix}-1 \amp 1 \\ 1 \amp 4\end{bmatrix}\begin{bmatrix}0.5^{60} \amp 0 \\ 0 \amp 1 \end{bmatrix}\begin{bmatrix}-0.8 \amp 0.2 \\ 0.2 \amp 0.2\end{bmatrix}</mrow>
					<mrow> \amp = \begin{bmatrix}0.8/2^{60}+0.2 \amp 0.2-0.2/2^{60} \\ 0.8-0.8/2^{60} \amp 0.2/2^{60} +0.8\end{bmatrix}</mrow>
				</md>.
				</p>
				<p>Now the probability we are interested in is the top entry of <m>A^{60}\begin{bmatrix}1\\0\end{bmatrix}</m>, which is <m>0.8/2^{60}+0.2 \approx 0.200000000000000000694</m>.  Thus there is approximately a <m>20\%</m> chance that the student will be studying one hour from now. </p>
			</solution>
		</example>
	</subsection>

	<xi:include href="4-2-exercises.ptx" />
</section>