<section xml:id="sec-Ortho" xmlns:xi="http://www.w3.org/2001/XInclude">

	<title>
		Orthogonality
	</title>

	<introduction>
		<p> You might have noticed that in the past few chapters we have not used the dot product for anything.  In this chapter we explore how the new tools we have been developing interact with the dot product.  In this chapter we return to our previous convention that "scalar" means "real number". </p>
		<p> You might want to take a moment to review the definition of the dot product, and the notion of orthogonality of vectors, from <xref ref="sec-DotProd"/>.</p>
	</introduction>

	<subsection>
		<title>
			Orthogonal and orthonormal bases
		</title>

		<definition>
			<p> Let <m>X = \{\vec{v_1}, \ldots, \vec{v_k}\}</m> be a finite collection of vectors in <m>\mathbb{R}^n</m>. </p>
			<p> The collection <m>X</m> is called an <em>orthogonal set</em> if whenever <m>i \neq j</m> we have <m>\vec{v_i} \cdot \vec{v_j} = 0</m>.</p>
			<p> The collection <m>X</m> is called an <em>orthonormal set</em> if it is an orthogonal set, and for every <m>i</m> we also have <m>\norm{\vec{v_i}} = 1</m>. </p>
		</definition>

		<example>
			<p> Some orthogonal sets, some orthonormal sets, some neither </p>
		</example>

		<note><p>If <m>\vec{v_1}, \ldots, \vec{v_k}</m> are an orthogonal set of vectors, and none of them is <m>\vec{0}</m>, then <m>\frac{1}{\norm{\vec{v_1}}}\vec{v_1}, \ldots, \frac{1}{\norm{\vec{v_k}}}\vec{v_k}</m> is an orthonormal set with the same span as <m>\vec{v_1}, \ldots, \vec{v_k}</m>.</p></note>

		<definition>
			<p> Let <m>S</m> be a subspace of <m>\mathbb{R}^n</m>.  An <em>orthogonal basis</em> for <m>S</m> is a basis for <m>S</m> that is also an orthogonal set.  An <em>orthonormal basis</em> for <m>S</m> is a basis for <m>S</m> that is also an orthonormal set.</p>
		</definition>

		<p> As this chapter develops we will see that if we have a subspace of <m>\mathbb{R}^n</m> then it is often more convenient to work with an orthonormal basis for that subspace than it is to work with an arbitrary basis.  In principle, to check that <m>B</m> is an orthogonal basis for a subspace <m>S</m> we need to check three things:
			<ol>
				<li><m>B</m> is an orthogonal set,</li>
				<li><m>B</m> is a linearly independent set,</li>
				<li><m>\span(B) = S</m>.</li>
			</ol>
		Fortunately, our next result tells us that if we check (1) we do not also need to check (2). </p>

		<theorem xml:id="thm-orthogonal-indep">
			<statement>
				<p> Let <m>X = \{\vec{v_1}, \ldots, \vec{v_k}\}</m> be an orthogonal set of vectors in <m>\mathbb{R}^n</m>.  Then <m>X</m> is linearly independent if and only if <m>\vec{v_j} \neq \vec{0}</m> for all <m>j</m>. </p>
			</statement>
			<proof>
				<p> Suppose first that <m>X</m> is linearly independent.  Then <m>\vec{0}</m> cannot be in <m>X</m>, by <xref ref="ex-zero-dependent" />.  So <m>\vec{v_j} \neq \vec{0}</m> for all <m>j</m>.</p>
				<p> Now for the harder direction, suppose that <m>\vec{v_j} \neq \vec{0}</m> for all <m>j</m>.  Suppose that we have scalars <m>c_1, \ldots, c_k</m> such that <m>c_1\vec{v_1} + \cdots + c_k\vec{v_k} = \vec{0}</m>.  Taking the dot product with <m>\vec{v_1}</m> on both sides of the equation we get:</p>
				<md>
					<mrow> 0 \amp = \vec{v_1} \cdot \vec{0} </mrow>
					<mrow> \amp = \vec{v_1} \cdot (c_1\vec{v_1} + c_2\vec{v_2} + \cdots + c_k\vec{v_k}) </mrow>
					<mrow> \amp = c_1(\vec{v_1}\cdot \vec{v_1}) + c_2(\vec{v_1}\cdot\vec{v_2}) + \cdots + c_k(\vec{v_1}\cdot\vec{v_k}) </mrow>
					<mrow> \amp = c_1\norm{\vec{v_1}}^2 + c_2(0) + \cdots + c_k(0) </mrow>
					<mrow> \amp = c_1\norm{\vec{v_1}}^2</mrow>
				</md>
				<p> In the calculation above the terms that became did so because of the assumption that <m>X</m> is an orthogonal set.  Now <m>\vec{v_1} \neq \vec{0}</m> implies that <m>\norm{\vec{v_1}}^2 \neq 0</m>, so we must have <m>c_1 = 0</m>.  Repeating this argument using the dot product with <m>\vec{v_2}</m> instead of <m>\vec{v_1}</m> shows that <m>c_2=0</m>, and so on.  Thus <m>c_1=c_2=\cdots = c_k = 0</m>, as required to show that <m>X</m> is a linearly independent set.</p>
			</proof>
		</theorem>

		<p> Recall that if <m>\vec{v_1}, \ldots, \vec{v_k}</m> form a basis for a subspace <m>S</m>, and if <m>\vec{w}</m> is a vector in <m>S</m>, then there are unique scalars <m>c_1, \ldots, c_k</m> such that <m>\vec{w} = c_1\vec{v_1} + \cdots + c_k\vec{v_k}</m>.  In general, to find <m>c_1, \ldots, c_k</m> involves solving a system of linear equations.  When the basis is orthogonal there is an easier way. </p>

		<theorem xml:id="thm-coordinates-orthogonal">
			<statement>
				<p> Let <m>S</m> be a subspace of <m>\mathbb{R}^n</m>, and suppose that <m>\vec{v_1}, \ldots, \vec{v_k}</m> form an orthogonal basis for <m>S</m>.  Then for any vector <m>\vec{w}</m> in <m>S</m>, the unique way to write <m>\vec{w} = c_1\vec{v_1} + \cdots + c_k\vec{v_k}</m> is with, for each <m>j</m>, <m>c_j = \frac{\vec{w} \cdot \vec{v_j}}{\vec{v_j} \cdot \vec{v_j}}</m>. </p>
			</statement>
			<proof>
				<p>The idea of the proof is very similar to the proof of <xref ref="thm-orthogonal-indep" />.  Suppose that we have written <m> \vec{w} = c_1\vec{v_1} + \cdots + c_k\vec{v_k}</m>.  If we consider any <m>j</m>, and take the dot product of both sides of the equation with <m>\vec{v_j}</m>, then we get:</p>
				<md>
					<mrow>\vec{w}\cdot\vec{v_j} \amp = \vec{v_j}\cdot(c_1\vec{v_1} + \cdots + c_k\vec{v_k}) </mrow>
					<mrow> \amp = c_1(\vec{v_1}\cdot\vec{v_j}) + \cdots + c_k(\vec{v_k} \cdot \vec{v_j}) </mrow>
					<mrow> \amp = c_j(\vec{v_j} \cdot \vec{v_j}) </mrow>
				</md>
				<p> All of the other terms in the calculation are <m>0</m> because the vectors <m>\vec{v_1}, \ldots, \vec{v_k}</m> are assumed to be orthogonal.  Now since <m>\vec{v_1}, \ldots, \vec{v_k}</m> are a basis for <m>S</m> they must be linearly independent, and in particular none of these vectors can be <m>\vec{0}</m>.  Thus <m>\vec{v_j} \cdot \vec{v_j} \neq 0</m>, so we can divide to conclude <m>c_j = \frac{\vec{w} \cdot \vec{v_j}}{\vec{v_j} \cdot \vec{v_j}}</m>, as desired. </p>
			</proof>
		</theorem>

		<example>
			<p>An example of finding coordinates, both the hard way adn the fast way. </p>
		</example>

		<note><p>If our basis <m>\vec{v_1}, \ldots, \vec{v_k}</m> is not just an orthogonal basis, but is actually an orthonormal basis, then the denominators in <xref ref="thm-coordinates-orthogonal" /> are all <m>1</m>, which makes the formulas for the coefficients in that case even simpler. </p></note>
	</subsection>

	<subsection>
		<title>
			The Gram-Schmidt algorithm
		</title>
		<p> Given a subspace <m>S</m> of <m>\mathbb{R}^n</m>, we know from <xref ref="thm-bases-exist" /> that there is a basis for <m>S</m>.  The next theorem gives us a method for transforming a basis for <m>S</m> into an orthogonal, or even orthonormal, basis for <m>S</m>.  We omit the proof, though you are encouraged to look back at the formulas in the algorithm after you read about orthogonal decompositions in <xref ref="sec-OrthoProj" />.</p>
		<theorem xml:id="thm-Gram-Schmidt">
			<title>Gram-Schmidt Algorithm</title>
			<statement>
				<p> Let <m>\vec{v_1}, \ldots, \vec{v_k}</m> be a linearly independent collection of vectors in <m>\mathbb{R}^n</m>.  Define new vectors as follows: </p>
				<md>
					<mrow>\vec{w_1} \amp = \vec{v_1}</mrow>
					<mrow>\vec{w_2} \amp = \vec{v_2} - \proj_{\vec{w_1}}(\vec{v_2})</mrow><mrow> \amp = \vec{v_2} - \left(\frac{\vec{w_1}\cdot\vec{v_2}}{\vec{w_1}\cdot\vec{w_1}}\right)\vec{w_1} </mrow>
					<mrow>\vec{w_3} \amp = \vec{v_3} - \proj_{\vec{w_1}}(\vec{v_3}) - \proj_{\vec{w_2}}(\vec{v_3}) </mrow><mrow>\amp = \vec{v_3} - \left(\frac{\vec{w_1}\cdot\vec{v_3}}{\vec{w_1}\cdot\vec{w_1}}\right)\vec{w_1} - \left(\frac{\vec{w_2}\cdot\vec{v_3}}{\vec{w_2}\cdot\vec{w_2}}\right)\vec{w_2}</mrow>
					<mrow>\vdots</mrow>
				</md>
				<p>Then <m>\vec{w_1}, \ldots, \vec{w_k}</m> form an orthogonal set of vectors, and for every <m>j</m> we have <me>\span(\vec{v_1}, \ldots, \vec{v_j}) = \span(\vec{w_1}, \ldots, \vec{w_j})</me>.</p>
				<p>In particular, if <m>\{\vec{v_1}, \ldots, \vec{v_k}\}</m> is a basis for a subspace <m>S</m>, then <m>\{\vec{w_1}, \ldots, \vec{w_k}\}</m> is an orthogonal basis for <m>S</m>, and <m>\left\{\frac{1}{\norm{\vec{w_1}}}\vec{w_1}, \ldots, \frac{1}{\norm{\vec{w_k}}}\vec{w_k}\right\}</m> is an orthonormal basis for <m>S</m>.</p>
			</statement>
		</theorem>

		<example>

		</example>
	</subsection>


	<xi:include href="5-1-exercises.ptx" />
</section>