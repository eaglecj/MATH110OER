<section xml:id="sec-OrthoMatrix" xmlns:xi="http://www.w3.org/2001/XInclude">

	<title>
		Orthogonal matrices
	</title>

	<subsection>
		<title>
			Definition and examples
		</title>

		<definition>
			<p> A square matrix <m>Q</m> is called an <em>orthogonal matrix</em> if the columns of <m>Q</m> are an orthonormal set.</p>
		</definition>

		<note>
			<p>Be careful: Despite the name, a matrix that has orthogonal columns is not necessarily an orthogonal matrix.  The definition of "orthogonal matrix" requires that the columns be <em>orthonormal</em>.</p>
		</note>

		<example>

		</example>

		<example>

		</example>
	</subsection>

	<subsection>
		<title>
			Properties of orthogonal matrices
		</title>

		<theorem xml:id="thm-orthogonal-matrix-inverse">
			<statement>
				<p> An <m>n \times n</m> matrix <m>Q</m> is an orthogonal matrix if and only if it is invertible and <m>Q^{-1} = Q^t</m>.</p>
			</statement>
			<proof>
				<p> Suppose first that <m>Q</m> is an orthogonal matrix.  By <xref ref="lem-multiply-dot-product" /> and <xref ref="def-matrix-multiply" /> the <m>(i,j)</m> entry of <m>Q^tQ</m> is the dot product of row <m>i</m> of <m>Q^t</m> and column <m>j</m> of <m>Q</m>.  Since row <m>i</m> of <m>Q^t</m> is column <m>i</m> of <m>Q</m>, and the columns of <m>Q</m> form an orthonormal set, we have that the <m>(i,j)</m> entry of <m>Q^tQ</m> is <m>1</m> when <m>i=j</m> and <m>0</m> when <m>i\neq j</m>.  That is, <m>Q^tQ = I_n</m>.  By <xref ref="thm-one-sided-inverse" /> this is enough to allow us to conclude that <m>Q</m> is invertible and <m>Q^{-1} = Q^t</m>. </p>

				<p> Now suppose that <m>Q^t = Q^{-1}</m>.  Then <m>Q^tQ = I_n</m>, so the <m>(i,j)</m> entry of <m>Q^tQ</m> is <m>1</m> when <m>i=j</m> and <m>0</m> when <m>i \neq j</m>.  As in the previous part of this proof, the <m>(i,j)</m> entry of <m>Q^tQ</m> is the dot product of column <m>i</m> of <m>Q</m> with column <m>j</m> of <m>Q</m>.  Thus the columns of <m>Q</m> are orthonormal, so <m>Q</m> is an orthogonal matrix.</p>
			</proof>
		</theorem>

		<example>

		</example>

		<corollary>
			<statement>
				<p> Suppose that <m>Q</m> is an orthogonal matrix.  Then the rows of <m>Q</m> form an orthonormal set of vectors.</p>
			</statement>
			<proof>
				<p> If <m>Q</m> is orthogonal then <m>Q^{-1} = Q^t</m> by <xref ref="thm-orthogonal-matrix-inverse" />, so by <xref ref="thm-inverse-properties"/>, <me> (Q^t)^{-1} = (Q^{-1})^t = (Q^t)^t</me>.  By <xref ref="thm-orthogonal-matrix-inverse" /> again this implies that <m>Q^t</m> is an orthogonal matrix.  Hence by definition the columns of <m>Q^t</m> form an orthonormal set.  Since the columns of <m>Q^t</m> are the rows of <m>Q</m>, we have that the rows of <m>Q</m> form an orthonormal set.</p>
			</proof>
		</corollary>

		<p>Recall that when we introduced linear transformations we asked that a linear transformation should interact nicely with vector addition and scalar multiplication, but we did not require that it interact well with the more "geometric" notions of dot products or lengths.  It turns out that orthogonal matrices are precisely the matrices of linear transformations that do preserve these geometric features of <m>\mathbb{R}^n</m>.</p>
		<theorem>
			<statement>
				<p> Let <m>Q</m> be an <m>n \times n</m> matrix.  The following are equivalent:
					<ol>
						<li><m>Q</m> is an orthogonal matrix.</li>
						<li>For every vector <m>\vec{v}</m> in <m>\mathbb{R}^n</m>, <m>\norm{Q\vec{v}} = \norm{\vec{v}}</m>.</li>
						<li>For all vectors <m>\vec{v}</m> and <m>\vec{w}</m> in <m>\mathbb{R}^n</m>, <m>(Q\vec{v}) \cdot (Q \vec{w}) = \vec{v} \cdot \vec{w}</m>.</li>
					</ol>
				</p>
			</statement>
			<proof>

			</proof>
		</theorem>

		<corollary xml:id="cor-orthogonal-preserve-angle">
			<statement>
				<p> Let <m>Q</m> be an orthogonal <m>n \times n</m> matrix.  Then for all vectors <m>\vec{v}</m> and <m>\vec{w}</m> in <m>\mathbb{R}^n</m>, the angle between <m>Q\vec{v}</m> and <m>Q\vec{w}</m> is the same as the angle between <m>\vec{v}</m> and <m>\vec{w}</m>.</p>
			</statement>
			<proof>

			</proof>
		</corollary>

		<note>
			<p>It is possible for a matrix to preserve all angles (in the sense of <xref ref="cor-orthogonal-preserve-angle" />) without being an orthogonal matrix.  One easy example of such a matrix is <m>\begin{bmatrix}2 \amp 0 \\ 0 \amp 2\end{bmatrix}</m>.</p>
		</note>

		<p> Finally, here are some additional properties that orthogonal matrices have.  The properties in this next theorem are not <em>equivalent</em> to a matrix being orthogonal, they are only <em>consequences</em> of a matrix being orthogonal. </p>

		<theorem>
			<statement>
				<p> Let <m>A</m> and <m>B</m> be orthogonal matrices.  Then:
				<ol>
					<li><m>AB</m> is an orthogonal matrix.</li>
					<li><m>A^{-1}</m> is an orthogonal matrix.</li>
					<li><m>A^t</m> is an orthogonal matrix.</li>
					<li><m>\det(A) = \pm 1</m>.</li>
					<li>Every eigenvalue <m>\lambda</m> of <m>A</m> in <m>\mathbb{C}</m> has <m>\abs{\lambda} = 1</m>.</li>
				</ol>
				</p>
			</statement>

			<proof>

			</proof>
		</theorem>


	</subsection>

	<subsection>
		<title>
			Application: Characterizing rotations of <m>\mathbb{R}^2</m>
		</title>

		<p> We have seen already that rotations of <m>\mathbb{R}^2</m> are linear transformations, and we have seen what their matrices look like.  As an application of the ideas of this chapter we give an abstract way of detecting whether or not a given <m>2 \times 2</m> matrix is the matrix of a rotation.  For any angle <m>\theta</m>, let <m>T_\theta</m> denote the linear transformation of <m>\mathbb{R}^2</m> that rotates vectors counter-clockwise by <m>\theta</m> radians.</p>
		
		<theorem>
			<statement>
				<p> Let <m>A</m> be a <m>2 \times 2</m> matrix.  The following are equivalent:
				<ol>
					<li>There is an angle <m>\theta</m> such that <m>A = [T_\theta]</m>.</li>
					<li><m>A</m> is an orthogonal matrix and <m>\det(A) = 1</m>.</li>
				</ol>
				</p>
			</statement>
			<proof>
				<p> First, suppose that (1) holds, so <m>A = [T_\theta] = \begin{bmatrix}\cos(\theta) \amp -\sin(\theta) \\ \sin(\theta) \amp \cos(\theta)\end{bmatrix}</m>.  Then the dot product of the columns of <m>A</m> is <me>\begin{bmatrix}\cos(\theta) \\ \sin(\theta)\end{bmatrix}\cdot \begin{bmatrix}-\sin(\theta)\\ \cos(\theta)\end{bmatrix} = \cos(\theta)(-\sin(\theta))+\sin(\theta)\cos(\theta) = 0</me>, and the lengths are <me>\norm{\begin{bmatrix}\cos(\theta) \\ \sin(\theta)\end{bmatrix}} = \sqrt{\cos^2(\theta) + \sin^2(\theta)} = 1</me> and <me>\norm{\begin{bmatrix}-\sin(\theta) \\ \cos(\theta)\end{bmatrix}} = \sqrt{(-\sin(\theta))^2 + \cos^2(\theta)} = 1</me>.  Thus <m>A</m> is an orthogonal matrix.  The determinant is <me>\det(A) = \det\begin{bmatrix}\cos(\theta) \amp -\sin(\theta) \\ \sin(\theta) \amp \cos(\theta)\end{bmatrix} = \cos^2(\theta) + \sin^2(\theta) = 1</me>.  We've thus shown that (2) holds. </p>

				<p> Now suppose that (2) holds, and that <m>A = \begin{bmatrix}a \amp b \\ c \amp d\end{bmatrix}</m>.  Since <m>A</m> is an orthogonal matrix, the vectors <m>\begin{bmatrix}a\\c\end{bmatrix}</m> and <m>\begin{bmatrix}b\\d\end{bmatrix}</m> are unit vectors in <m>\mathbb{R}^2</m>, which means that (in standard position) they end at points on the unit circle.  There are therefore angles <m>\phi</m> and <m>\psi</m> such that <m>\begin{bmatrix}a\\c\end{bmatrix} = \begin{bmatrix}\cos(\phi) \\ \sin(\phi)\end{bmatrix}</m> and <m>\begin{bmatrix}b\\d\end{bmatrix} = \begin{bmatrix}\cos(\psi) \\ \sin(\psi)\end{bmatrix}</m>.  More than this, the vectors <m>\begin{bmatrix}a\\c\end{bmatrix}</m> and <m>\begin{bmatrix}b\\d\end{bmatrix}</m> must be orthogonal, so <m>\psi = \phi \pm \pi/2</m>.</p>

				<p>If <m>\psi = \phi+\pi/2</m> then <m>\cos(\psi) = \cos(\phi+\pi/2) = -\sin(\phi)</m> and <m>\sin(\psi) = \sin(\phi+\pi/2) = \cos(\phi)</m>.  In this case we get 
				<me>A = \begin{bmatrix}\cos(\phi) \amp \cos(\psi) \\ \sin(\phi) \amp \sin(\psi)\end{bmatrix} = \begin{bmatrix}\cos(\phi) \amp -\sin(\phi) \\ \sin(\phi) \amp \cos(\phi)\end{bmatrix} = [T_\phi]</me>.
				</p>

				<p> The other case is <m>\psi = \phi-\pi/2</m>.  In this case <m>\cos(\psi) = \cos(\phi-\pi/2) = \sin(\phi)</m> and <m>\sin(\psi) = \sin(\phi - \pi/2) = -\cos(\phi)</m>.  Then we have
				<me>A = \begin{bmatrix}\cos(\phi) \amp \cos(\psi) \\ \sin(\phi) \amp \sin(\psi) \end{bmatrix} = \begin{bmatrix}\cos(\phi) \amp \sin(\phi) \\ \sin(\phi) \amp -\cos(\phi)\end{bmatrix}</me>. 
				This does not look like the matrix of a rotation, but there's also a part of the hypothesis (2) that we haven't used yet: <m>\det(A) = 1</m>.  If we calculate the determinant of the matrix we obtained in this case we get <me>\det\begin{bmatrix}\cos(\phi) \amp \sin(\phi) \\ \sin(\phi) \amp -\cos(\phi)\end{bmatrix} = -\cos^2(\phi) - \sin^2(\phi) = -1</me>.  Since this contradicts the assumption (2), this case (<m>\psi = \phi-\pi/2)</m>) is impossible.  We therefore were in the previous case, and we have <m>A = [T_\phi]</m>. </p>
			</proof>
		</theorem>

		<example>

		</example>
	</subsection>

	<xi:include href="5-2-exercises.ptx" />
</section>