<section xml:id="sec-OrthoMatrix" xmlns:xi="http://www.w3.org/2001/XInclude">

	<title>
		Orthogonal matrices
	</title>

	<subsection>
		<title>
			Definition and examples
		</title>

		<definition>
		</definition>

		<note>
			<p>Be careful: Despite the name, a matrix that has orthogonal columns is not necessarily an orthogonal matrix.  The definition of "orthogonal matrix" requires that the columns be <em>orthonormal</em>.</p>
		</note>

		<example>

		</example>

		<example>

		</example>
	</subsection>

	<subsection>
		<title>
			Properties of orthogonal matrices
		</title>

		<theorem xml:id="thm-orthogonal-matrix-inverse">
			<statement>
				<p> An <m>n \times n</m> matrix <m>Q</m> is an orthogonal matrix if and only if it is invertible and <m>Q^{-1} = Q^t</m>.</p>
			</statement>
		</theorem>

		<example>

		</example>

		<corollary>
			<statement>
				<p> Suppose that <m>Q</m> is an orthogonal matrix.  Then the rows of <m>Q</m> form an orthonormal set of vectors.</p>
			</statement>
			<proof>
				<p> If <m>Q</m> is orthogonal then <m>Q^{-1} = Q^t</m> by <xref ref="thm-orthogonal-matrix-inverse" />, so by <xref ref="thm-inverse-properties"/>, <me> (Q^t)^{-1} = (Q^{-1})^t = (Q^t)^t</me>.  By <xref ref="thm-orthogonal-matrix-inverse" /> again this implies that <m>Q^t</m> is an orthogonal matrix.  Hence by definition the columns of <m>Q^t</m> form an orthonormal set.  Since the columns of <m>Q^t</m> are the rows of <m>Q</m>, we have that the rows of <m>Q</m> form an orthonormal set.</p>
			</proof>
		</corollary>

		<p>Recall that when we introduced linear transformations we asked that a linear transformation should interact nicely with vector addition and scalar multiplication, but we did not require that it interact well with the more "geometric" notions of dot products or lengths.  It turns out that orthogonal matrices are precisely the matrices of linear transformations that do preserve these geometric features of <m>\mathbb{R}^n</m>.</p>
		<theorem>
			<statement>
				<p> Let <m>Q</m> be an <m>n \times n</m> matrix.  The following are equivalent:
					<ol>
						<li><m>Q</m> is an orthogonal matrix.</li>
						<li>For every vector <m>\vec{v}</m> in <m>\mathbb{R}^n</m>, <m>\norm{Q\vec{v}} = \norm{\vec{v}}</m>.</li>
						<li>For all vectors <m>\vec{v}</m> and <m>\vec{w}</m> in <m>\mathbb{R}^n</m>, <m>(Q\vec{v}) \cdot (Q \vec{w}) = \vec{v} \cdot \vec{w}</m>.</li>
					</ol>
				</p>
			</statement>
			<proof>

			</proof>
		</theorem>

		<corollary xml:id="cor-orthogonal-preserve-angle">
			<statement>
				<p> Let <m>Q</m> be an orthogonal <m>n \times n</m> matrix.  Then for all vectors <m>\vec{v}</m> and <m>\vec{w}</m> in <m>\mathbb{R}^n</m>, the angle between <m>Q\vec{v}</m> and <m>Q\vec{w}</m> is the same as the angle between <m>\vec{v}</m> and <m>\vec{w}</m>.</p>
			</statement>
			<proof>

			</proof>
		</corollary>

		<note>
			<p>It is possible for a matrix to preserve all angles (in the sense of <xref ref="cor-orthogonal-preserve-angle" />) without being an orthogonal matrix.  One easy example of such a matrix is <m>\begin{bmatrix}2 \amp 0 \\ 0 \amp 2\end{bmatrix}</m>.</p>
		</note>

		<p> Finally, here are some additional properties that orthogonal matrices have.  The properties in this next theorem are not <em>equivalent</em> to a matrix being orthogonal, they are only <em>consequences</em> of a matrix being orthogonal. </p>

		<theorem>
			<statement>
				<p> Let <m>A</m> and <m>B</m> be orthogonal matrices.  Then:
				<ol>
					<li><m>AB</m> is an orthogonal matrix.</li>
					<li><m>A^{-1}</m> is an orthogonal matrix.</li>
					<li><m>A^t</m> is an orthogonal matrix.</li>
					<li><m>\det(A) = \pm 1</m>.</li>
					<li>Every eigenvalue <m>\lambda</m> of <m>A</m> in <m>\mathbb{C}</m> has <m>\abs{\lambda} = 1</m>.</li>
				</ol>
				</p>
			</statement>

			<proof>

			</proof>
		</theorem>


	</subsection>

	<subsection>
		<title>
			Application: Characterizing rotations of <m>\mathbb{R}^2</m>
		</title>

		<p> We have seen already that rotations of <m>\mathbb{R}^2</m> are linear transformations, and we have seen what their matrices look like.  As an application of the ideas of this chapter we give an abstract way of detecting whether or not a given <m>2 \times 2</m> matrix is the matrix of a rotation.  For any angle <m>\theta</m>, let <m>T_\theta</m> denote the linear transformation of <m>\mathbb{R}^2</m> that rotates vectors counter-clockwise by <m>\theta</m> radians.</p>
		
		<theorem>
			<statement>
				<p> Let <m>A</m> be a <m>2 \times 2</m> matrix.  The following are equivalent:
				<ol>
					<li>There is an angle <m>\theta</m> such that <m>A = [T_\theta]</m>.</li>
					<li><m>A</m> is an orthogonal matrix and <m>\det(A) = 1</m>.</li>
				</ol>
				</p>
			</statement>
			<proof>

			</proof>
		</theorem>

		<example>

		</example>
	</subsection>

	<xi:include href="5-2-exercises.ptx" />
</section>